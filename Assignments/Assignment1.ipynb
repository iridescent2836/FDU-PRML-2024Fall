{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDU PRML 2024 Fall Assignment 1\n",
    "\n",
    "Name: `<your name>`\n",
    "\n",
    "Student ID: `<your student id>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions and complete the following exercises using PyTorch.\n",
    "\n",
    "## 1. Basic Operations of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "my_first_tensor = None  # TODO: assign a tensor of shape (3, 4) with all elements equal to 1.0\n",
    "\n",
    "my_second_tensor = None  # TODO: assign a random tensor of shape (3, 4) with all elements sampled from a standard normal distribution\n",
    "\n",
    "their_matrix_product = None  # TODO: compute the matrix product of my_first_tensor and the transpose of my_second_tensor (There are multiple ways to do this. Just pick one you like.)\n",
    "\n",
    "some_meaningless_concatenation = None  # TODO: concatenate my_first_tensor and my_second_tensor along the first dimension. (Maybe you should check the documentation of torch.cat)\n",
    "\n",
    "some_meaningless_stack = None  # TODO: stack 5 copies of my_first_tensor along a newly created dimension. (Maybe you should check the documentation of torch.stack)\n",
    "\n",
    "# What is the shape of some_meaningless_stack? Can you imagine the geometric interpretation of stacking 5 matrices of shape (3, 4) along the first dimension?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple logistic regression\n",
    "\n",
    "There are 4 core components in Pytorch training process: **model**, **loss function**, **optimizer** and **data loader**. In this part, we will implement a simple logistic regression model to illustrate them.\n",
    "\n",
    "### 2.1 Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear layer for logitstic regression\n",
    "\n",
    "class Linear(torch.nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tpass\n",
    "\t\t# TODO: initialize the weight and bias of the linear layer.\n",
    "  \n",
    "\tdef forward(self, x):\n",
    "\t\tpass\n",
    "\t\t# TODO: implement the forward function of a linear layer.\n",
    "\n",
    "\n",
    "def loss_function(y_pred, y):\n",
    "    # TODO: implement the loss function of logistic regression.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data\n",
    "\n",
    "In real world, we usually have to deal with large-scale datasets. However, in this assignment, we will use synthetic data to illustrate the training process. The synthetic data is generated by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data for binary classification\n",
    "\n",
    "num_samples = 100\n",
    "num_features = 2\n",
    "\n",
    "x_0 = torch.randn(num_samples, num_features) + torch.tensor([2.0, 2.0])\n",
    "y_0 = torch.zeros(num_samples)\n",
    "\n",
    "x_1 = torch.randn(num_samples, num_features) + torch.tensor([-2.0, -2.0])\n",
    "y_1 = torch.ones(num_samples)\n",
    "\n",
    "x = torch.cat([x_0, x_1], dim=0)\n",
    "y = torch.cat([y_0, y_1], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset to feed into the model\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t# TODO: implement the __getitem__ function.\n",
    "\t\tpass\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t# TODO: implement the __len__ function.\n",
    "\t\tpass\n",
    "\n",
    "dataset = MyDataset(x, y)\n",
    "dataloder = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Linear(num_features, 1)\n",
    "\n",
    "optimizer = None  # TODO: initialize an optimizer of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together\n",
    "\n",
    "Since this is just a toy experiment, we do not need validation.\n",
    "\n",
    "In the following code, we expect to see the training loss decreasing to 0.001 or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "for epoch in range(100):\n",
    "\tfor batch_x, batch_y in dataloder:\n",
    "\t\t# TODO: implement the training loop.\n",
    "\t\tpass\n",
    "  \n",
    "\tif epoch % 10 == 0:\n",
    "\t\tprint('epoch: {}, loss: {}'.format(epoch, loss.item()))\n",
    "  \n",
    "# save the model\n",
    "\n",
    "torch.save(my_model.state_dict(), 'my_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST Classification\n",
    "\n",
    "In this section, you will use PyTorch to implement a multi-layer perceptron (MLP) model for classifying handwritten digits using the MNIST dataset.\n",
    "\n",
    "\n",
    "1. Data Loading and Preprocessing:\n",
    "   - Utilize the `torchvision.datasets` module to load the MNIST dataset.\n",
    "   - Apply necessary transformations (like `ToTensor` and `Normalize`) to prepare the data for model training. These transformations ensure the data has the correct format and scales, helping with model convergence.\n",
    "   - Use a DataLoader with a suitable batch size to efficiently manage data feeding into the model.\n",
    "\n",
    "2. Architecture:\n",
    "   - Define a simple MLP model with PyTorch's `torch.nn.Module`. A suggested architecture is:\n",
    "     - An input layer that takes the flattened 28x28 pixel values (784 features).\n",
    "     - One or more hidden layers with ReLU activations for non-linearity.\n",
    "     - An output layer with softmax activation for multi-class classification.\n",
    "   - Make sure to initialize the model appropriately, especially if you're stacking multiple layers.\n",
    "\n",
    "3. Training:\n",
    "   - Set up an optimizer (like `Adam` or `SGD`) to minimize the model's error during training. You will also need a loss function, such as `CrossEntropyLoss`, which is well-suited for classification tasks.\n",
    "   - Write a training loop that performs the following steps:\n",
    "     - Forward pass: Feed batches through the model to obtain predictions.\n",
    "     - Compute the loss by comparing predictions with true labels.\n",
    "     - Backward pass: Calculate gradients for each model parameter.\n",
    "     - Update the model weights using the optimizer.\n",
    "   - Periodically log or print the training loss to track progress.\n",
    "\n",
    "4. Evaluation:\n",
    "   - After training, evaluate your model on the test set.\n",
    "   - Compute and print the accuracy metric, and optionally, create a confusion matrix to analyze classification errors.\n",
    "\n",
    "\n",
    "MNIST: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
