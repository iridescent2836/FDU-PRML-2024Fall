{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa9e6fb23a5356",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "In this exercise, you will complete the implementation of a Decision Tree classifier based on our simple `fduml` framework. We have written most of the code for you already, and you only need to fill in the most essential parts marked in `TODO`. We have also prepared several test cases for you to check if your code works correctly. Furthermore, you can also test the accuracy of your code by comparing its output with the output of Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc42f1d4c0679bec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Auto reload external modules, which means you can modify the code of our fduml implementation without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbac084f253b88a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b847afee6a108d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implement and test (40 points)\n",
    "\n",
    "We have prepared several test cases for you to check if your code works correctly. After you write your own implementation, try the following code for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df4695e706b454e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fduml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864192717f0f9a52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2 -1]\n",
      " [-1 -1]\n",
      " [-1 -2]\n",
      " [ 1  1]\n",
      " [ 1  2]\n",
      " [ 2  1]]\n"
     ]
    }
   ],
   "source": [
    "from fduml.tree.tests.test_decision_tree import test_dt_classification\n",
    "test_dt_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9f980",
   "metadata": {},
   "source": [
    "## Load data and fit the model (40 points)\n",
    "\n",
    "Inside the `data` directory we have prepared a classification dataset, split into training and test sets. In this part, you will load the data and fit the model to the training data. Then, you will evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc6885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\n",
      "\n",
      "            ph    Hardness       Solids  Chloramines     Sulfate  \\\n",
      "0     8.618654  257.595883  11595.35498     6.399933   -1.000000   \n",
      "1    -1.000000  199.942222  25973.32663     6.490994  336.040741   \n",
      "2     4.923179  208.406673  15990.14923     5.648146  349.655175   \n",
      "3     7.617524  179.596189  30308.23118     6.952617  329.422414   \n",
      "4     8.891674  184.869606  41801.44184     3.409576  337.047108   \n",
      "...        ...         ...          ...          ...         ...   \n",
      "2616  4.814136  205.214041  17650.40505     8.121080  350.487939   \n",
      "2617 -1.000000  221.391974  27979.73622     6.572390  349.624553   \n",
      "2618  5.596730  229.295098  44652.36387     6.500953  323.999049   \n",
      "2619  7.079304  137.007355  24282.15477     5.705693  433.633900   \n",
      "2620 -1.000000  255.953599  15097.02406     8.482421  361.971419   \n",
      "\n",
      "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0       343.740007       15.331166        75.687401   4.141342           0  \n",
      "1       344.970363       12.640414        46.854524   3.151768           0  \n",
      "2       404.405763       11.403372        84.525775   3.329601           1  \n",
      "3       355.425532       13.400760        66.223591   3.698317           0  \n",
      "4       461.076821       13.715504        42.078122   4.522599           0  \n",
      "...            ...             ...              ...        ...         ...  \n",
      "2616    414.030709       10.999416        47.402666   5.190852           0  \n",
      "2617    355.781983        9.986355        81.963485   3.439623           0  \n",
      "2618    424.257013       12.361827        40.412098   3.826158           0  \n",
      "2619    342.725318        9.113945        88.328605   5.553174           0  \n",
      "2620    471.477419       14.570932        40.287298   3.227941           1  \n",
      "\n",
      "[2621 rows x 10 columns]\n",
      "test_data\n",
      "\n",
      "           ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
      "0    8.570129  200.071875   9782.344284     5.661697   -1.000000   \n",
      "1    6.106760  211.454489  39430.307820     8.316897  348.776719   \n",
      "2   -1.000000  215.977859  17107.224230     5.607060  326.943978   \n",
      "3    4.405327  169.742537  15039.710410     6.308198  352.917733   \n",
      "4    3.975753  135.891978  17430.841940     6.305788  373.486425   \n",
      "..        ...         ...           ...          ...         ...   \n",
      "650  8.304335  155.569941  18253.181100    11.448469  329.100636   \n",
      "651  4.487517  199.051403  36860.468900     9.751561  338.049427   \n",
      "652 -1.000000  217.944979  37820.047330     8.299339   -1.000000   \n",
      "653  7.130099  275.679780   9480.617796     8.415948  295.618838   \n",
      "654  6.655605  216.441171  20571.845710     8.984237   -1.000000   \n",
      "\n",
      "     Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0      511.161511       11.856089        66.048413   4.604405           0  \n",
      "1      389.591440       12.896953        85.358049   3.924967           0  \n",
      "2      436.256194       14.189062        59.855476   5.459251           0  \n",
      "3      424.251162       14.441754        79.169597   4.086867           0  \n",
      "4      344.398912       15.624310        68.370968   3.666824           1  \n",
      "..            ...             ...              ...        ...         ...  \n",
      "650    380.323440       13.755926        35.057030   4.181254           1  \n",
      "651    390.566945        8.123470        75.668785   2.765722           0  \n",
      "652    367.570082       15.421034        36.446614   2.994780           0  \n",
      "653    383.455068       18.322879        94.416301   1.986192           0  \n",
      "654    336.831455       17.745996        71.704305   2.951645           0  \n",
      "\n",
      "[655 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the water potability dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('../Exercise2/data/water_potability_train.csv')\n",
    "test_data = pd.read_csv('../Exercise2/data/water_potability_test.csv')\n",
    "\n",
    "print(\"train_data\\n\")\n",
    "print(train_data)\n",
    "\n",
    "print(\"test_data\\n\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a DecisionTreeClassifier to the water potability train set\n",
    "from fduml.tree import DecisionTreeClassifier\n",
    "from time import time\n",
    "\n",
    "X_train = train_data.drop(columns=['Potability'])\n",
    "y_train = train_data['Potability']\n",
    "\n",
    "X_test = test_data.drop(columns=['Potability'])\n",
    "y_test = test_data['Potability']\n",
    "\n",
    "start_time = time()\n",
    "dt_classfier =  DecisionTreeClassifier(criterion=\"info_gain\", random_state=0)\n",
    "dt_classfier.fit(X_train.values, y_train.values)\n",
    "\n",
    "y_pred = dt_classfier.predict(X_test.values)\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c07ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier: 0.59\n",
      "Classification took 157.98 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the DecisionTreeClassifier on the water potability test set\n",
    "accuracy = accuracy_score(y_test.values, y_pred)\n",
    "print(f'Accuracy of the Decision Tree Classifier: {accuracy:.2f}')\n",
    "print(f'Classification took {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865aa4552324124",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Compare with Sklearn (20 points)\n",
    "\n",
    "Since the interface of our `fduml` is the same as that of sklearn, you can easily compare the output of your implementation with that of sklearn. In this part, try to generate test data and compare the accuracy and running time of your implementation with that of sklearn. You can use the following code for comparison.\n",
    "\n",
    "In the conclusion part, try to answer the following questions:\n",
    "\n",
    "- Is the accuracy of your implementation the same as that of sklearn? If not, what can be the reason?\n",
    "\n",
    "- Is the running time of your implementation the same as that of sklearn? If not, what can be the reason?\n",
    "\n",
    "- If there is any special thing you want to mention, please write it down.\n",
    "\n",
    "Note that we do not require you to match the accuracy and running time of sklearn (which can be quite difficult), but you should be able to explain the reason if they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7521a793b9a5488",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree classifier: 0.61\n",
      "Classification took 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Create and fit the Decision Tree classifier\n",
    "start_time = time()\n",
    "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "end_time = time()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Accuracy of the Decision Tree classifier: {accuracy:.2f}')\n",
    "print(f'Classification took {end_time - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ef0501ed482fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55574ca4",
   "metadata": {},
   "source": [
    "#### Is the accuracy of your implementation the same as that of sklearn? If not, what can be the reason?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4c62a",
   "metadata": {},
   "source": [
    "\n",
    "  Yes, the accuracy of my implementation is basically consistent with that of sklearn on the same test set. My accracy is 0.59 and the accuracy of sklearn is 0.61.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83fa38",
   "metadata": {},
   "source": [
    "\n",
    "#### Is the running time of your implementation the same as that of sklearn? If not, what can be the reason?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a615b7d",
   "metadata": {},
   "source": [
    "\n",
    "  No, my implementation took 157.98 seconds while sklearn only took 0.03 seconds. Sklearn is 5266 times faster than mine :-)<br>\n",
    "  Here are the reasons why scikit-learn is so fast:\n",
    "\n",
    "- Cython and C/C++ Integration:\n",
    "Cython: Many parts of scikit-learn are written in Cython, which is a superset of Python that allows you to write C-like performance-critical code while retaining Pythonâ€™s syntax. This makes functions execute faster than pure Python.\n",
    "C/C++ Integration: The most computationally intensive parts, like matrix operations and numerical computations, are often implemented using low-level C or C++ libraries. This allows scikit-learn to take advantage of the speed of these lower-level languages.<br><br>\n",
    "- Use of Optimized Libraries:\n",
    "scikit-learn leverages highly optimized numerical libraries like NumPy and SciPy, which themselves rely on low-level libraries such as BLAS (Basic Linear Algebra Subprograms) and LAPACK. These libraries are highly optimized for matrix operations and numerical computations.\n",
    "By using these libraries, scikit-learn takes advantage of parallelized and optimized algorithms for common operations like matrix multiplication, solving systems of linear equations, etc.<br><br>\n",
    "- Efficient Algorithms:\n",
    "scikit-learn implements highly efficient algorithms, including those for decision trees, SVMs, and ensemble methods like random forests. The algorithms are well-researched, and they are implemented with a focus on efficiency.\n",
    "The library also uses optimizations like data structures tailored for fast access (e.g., KD-trees for nearest neighbor searches).<br><br>\n",
    "- Parallel Processing:\n",
    "For many of its models, scikit-learn can use parallel processing to leverage multiple CPU cores, which can significantly speed up training and prediction, especially with ensemble methods like random forests or gradient boosting.<br><br>\n",
    "- Compiled Code Execution:\n",
    "By relying on compiled code through Cython or external libraries, scikit-learn avoids the Global Interpreter Lock (GIL) bottleneck of Python. This allows it to execute more efficiently, especially for tasks that require heavy computation.\n",
    "This combination of optimized low-level code, efficient algorithms, and parallelism makes scikit-learn a powerful and fast tool for machine learning, even though its API is written in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
